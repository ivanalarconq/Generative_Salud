{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanalarconq/Generative_Salud/blob/main/Clase_0_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![banner](https://portal.insnsb.gob.pe/investigacion/wp-content/uploads/IA_AVANZADO_3.png)\n"
      ],
      "metadata": {
        "id": "_8Wn01F9hlnF"
      },
      "id": "_8Wn01F9hlnF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ©º Clase 0: Procesamiento bÃ¡sico de Texto en IA\n",
        "\n",
        "**INSTITUTO NACIONAL DE SALUD DEL NIÃ‘O SAN BORJA**\n",
        "\n",
        "__Creadores de Contenido:__ Sebastian Puruguay\n",
        "\n",
        "__Revisor de Contenido:__ Carlos Vasquez\n",
        "\n",
        "__Instructores:__ Carlos Vasquez, Sebastian Puruguay\n"
      ],
      "metadata": {
        "id": "FLjzR8rnhpZc"
      },
      "id": "FLjzR8rnhpZc"
    },
    {
      "cell_type": "markdown",
      "id": "949609d3",
      "metadata": {
        "id": "949609d3"
      },
      "source": [
        "# Procesamiento BÃ¡sico de Texto para IA en Salud\n",
        "## Clase PrÃ¡ctica: Transformando Texto ClÃ­nico en Datos Analizables\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ **Objetivos de esta clase**\n",
        "- Entender quÃ© es el **texto crudo** y por quÃ© necesita procesamiento\n",
        "- Dominar las tÃ©cnicas fundamentales de **limpieza y preprocesamiento**\n",
        "- Implementar **tokenizaciÃ³n** y **normalizaciÃ³n**\n",
        "- Convertir texto en **representaciones numÃ©ricas** que la IA entienda\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“‹ **Estructura de la clase**\n",
        "1. **Texto Crudo**: CÃ³mo llegan los datos clÃ­nicos\n",
        "2. **Limpieza y Preprocesamiento**: Eliminar ruido innecesario\n",
        "3. **TokenizaciÃ³n**: Dividir el texto en piezas analizables\n",
        "4. **NormalizaciÃ³n**: Estandarizar variaciones\n",
        "5. **RepresentaciÃ³n NumÃ©rica**: De texto a nÃºmeros\n",
        "6. **AnÃ¡lisis Final**: AplicaciÃ³n prÃ¡ctica integrada\n",
        "\n",
        "â±ï¸ **DuraciÃ³n esperada**: 45â€“60 minutos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f4afd7",
      "metadata": {
        "id": "39f4afd7"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª PASO 1 â€” TEXTO CRUDO: LA REALIDAD CLÃNICA\n",
        "\n",
        "### Â¿QuÃ© es texto crudo?\n",
        "El **texto crudo (raw text)** es exactamente cÃ³mo llega la informaciÃ³n clÃ­nica:\n",
        "- Notas mÃ©dicas sin procesar\n",
        "- Historiales hospitales\n",
        "- Reportes de laboratorio\n",
        "- Textos con mayÃºsculas, puntuaciÃ³n, nÃºmeros, sÃ­mbolos\n",
        "\n",
        "### ðŸŽ¯ Punto clave:\n",
        "> **\"Para la computadora, esto es solo una cadena de caracteres sin significado.\"**\n",
        "\n",
        "Veamos un ejemplo realista:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80138037",
      "metadata": {
        "id": "80138037"
      },
      "outputs": [],
      "source": [
        "# TEXTO CRUDO: Ejemplo de nota clÃ­nica real\n",
        "texto_crudo = \"\"\"\n",
        "Paciente masculino de 68 aÃ±os con antecedente de HTA y DM2.\n",
        "Refiere fiebre de 39Â°C, tos productiva y dificultad respiratoria.\n",
        "Se realiza TC de tÃ³rax: infiltrado bilateral. SpO2: 92%.\n",
        "DiagnÃ³stico: NeumonÃ­a Adquirida en la Comunidad (NAC).\n",
        "Tratamiento: Ceftriaxona 2g IV c/12h + Azitromicina 500mg VO.\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEXTO CRUDO (SIN PROCESAMIENTO):\")\n",
        "print(\"=\"*60)\n",
        "print(texto_crudo)\n",
        "print(f\"\\nLongitud: {len(texto_crudo)} caracteres\")\n",
        "print(f\"Contiene mayÃºsculas: âœ“\")\n",
        "print(f\"Contiene sÃ­mbolos y nÃºmeros: âœ“\")\n",
        "print(f\"Contiene espacios en blanco: âœ“\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fc131e8",
      "metadata": {
        "id": "6fc131e8"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª PASO 2 â€” NORMALIZACIÃ“N BÃSICA: ESTANDARIZAR MAYÃšSCULAS\n",
        "\n",
        "### Â¿Por quÃ© normalizar?\n",
        "La IA podrÃ­a interpretar **\"Paciente\"** y **\"paciente\"** como palabras diferentes:\n",
        "- Aumenta la dimensionalidad innecesariamente\n",
        "- Reduce la capacidad de generalizaciÃ³n\n",
        "- Consume mÃ¡s memoria\n",
        "\n",
        "### ðŸŽ¯ SoluciÃ³n: Convertir todo a minÃºsculas\n",
        "> **\"La IA no distingue mayÃºsculas. Reducimos variabilidad manteniendo informaciÃ³n clÃ­nica.\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d14591",
      "metadata": {
        "id": "b2d14591"
      },
      "outputs": [],
      "source": [
        "# PASO 2: ConversiÃ³n a minÃºsculas\n",
        "texto_normalizado = texto_crudo.lower()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DESPUÃ‰S DE NORMALIZACIÃ“N (MINÃšSCULAS):\")\n",
        "print(\"=\"*60)\n",
        "print(texto_normalizado)\n",
        "print(f\"\\nâœ“ Texto estandarizado\")\n",
        "print(f\"âœ“ Variabilidad reducida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "497d1a34",
      "metadata": {
        "id": "497d1a34"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª PASO 3 â€” LIMPIEZA: ELIMINAR RUIDO\n",
        "\n",
        "### Â¿QuÃ© es \"ruido\"?\n",
        "- **SÃ­mbolos innecesarios**: Â°, :, (, ), etc.\n",
        "- **NÃºmeros que no aportan valor semÃ¡ntico**: 68, 2, 39, etc.\n",
        "- **Espacios en blanco excesivos**\n",
        "\n",
        "### âš ï¸ IMPORTANTE - Cuidado mÃ©dico:\n",
        "> **\"Eliminamos ruido, NO informaciÃ³n clÃ­nica.\"**\n",
        "\n",
        "En este contexto, mantenemos:\n",
        "- âœ“ AcrÃ³nimos: **HTA**, **DM2**, **NAC** (valores mÃ©dicos)\n",
        "- âœ“ NÃºmeros crÃ­ticos SOLO si los procesamos numÃ©ricamente despuÃ©s\n",
        "- âœ— PuntuaciÃ³n: Salvo guiones en tÃ©rminos compuestos\n",
        "\n",
        "Para esta prÃ¡ctica, usaremos **expresiones regulares** para remover sÃ­mbolos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48453f8",
      "metadata": {
        "id": "f48453f8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# PASO 3: Eliminar sÃ­mbolos y caracteres especiales\n",
        "# Mantener: letras, nÃºmeros, espacios y Ã±\n",
        "texto_limpio = re.sub(r\"[^a-zÃ¡Ã©Ã­Ã³ÃºÃ±0-9\\s]\", \"\", texto_normalizado)\n",
        "\n",
        "# Eliminar espacios en blanco mÃºltiples\n",
        "texto_limpio = re.sub(r\"\\s+\", \" \", texto_limpio).strip()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DESPUÃ‰S DE LIMPIEZA (ELIMINACIÃ“N DE SÃMBOLOS):\")\n",
        "print(\"=\"*60)\n",
        "print(texto_limpio)\n",
        "print(f\"\\nâœ“ SÃ­mbolos removidos: Â° : ( ) , . â€”\")\n",
        "print(f\"âœ“ Espacios normalizados\")\n",
        "print(f\"âœ“ InformaciÃ³n clÃ­nica preservada: HTA, DM2, NAC, etc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faead247",
      "metadata": {
        "id": "faead247"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª PASO 4 â€” TOKENIZACIÃ“N: DIVIDIR EN PIEZAS ANALIZABLES\n",
        "\n",
        "### Â¿QuÃ© es tokenizaciÃ³n?\n",
        "Es dividir el texto en **tokens** (unidades mÃ­nimas de anÃ¡lisis):\n",
        "- **Token mÃ¡s simple**: Palabra (separada por espacios)\n",
        "- Otros tipos: CarÃ¡cter, sub-palabra, oraciÃ³n\n",
        "\n",
        "### ðŸŽ¯ Frase clave para esta clase:\n",
        "> **\"AquÃ­ la mÃ¡quina empieza a ver el texto como piezas analizables, no como una cadena monolÃ­tica.\"**\n",
        "\n",
        "### Â¿Por quÃ© es importante?\n",
        "La IA no puede procesar texto completo. Necesita:\n",
        "- RepresentaciÃ³n estructurada\n",
        "- Mapeo a Ã­ndices numÃ©ricos\n",
        "- AnÃ¡lisis individual y contextual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b94a27e",
      "metadata": {
        "id": "0b94a27e"
      },
      "outputs": [],
      "source": [
        "# PASO 4: TokenizaciÃ³n (por palabra)\n",
        "tokens = texto_limpio.split()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DESPUÃ‰S DE TOKENIZACIÃ“N:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"NÃºmero de tokens: {len(tokens)}\\n\")\n",
        "print(\"Tokens generados:\")\n",
        "print(tokens)\n",
        "\n",
        "# AnÃ¡lisis de los tokens\n",
        "print(f\"\\nðŸ“Š ESTADÃSTICAS:\")\n",
        "print(f\"   â€¢ Total de tokens: {len(tokens)}\")\n",
        "print(f\"   â€¢ Token mÃ¡s frecuente: \", end=\"\")\n",
        "from collections import Counter\n",
        "counter = Counter(tokens)\n",
        "print(f\"'{counter.most_common(1)[0][0]}' ({counter.most_common(1)[0][1]} veces)\")\n",
        "print(f\"   â€¢ Vocabulario Ãºnico: {len(set(tokens))} palabras diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b0047ac",
      "metadata": {
        "id": "0b0047ac"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª PASO 5 â€” ELIMINACIÃ“N DE STOPWORDS: FILTRAR RUIDO SINTÃCTICO\n",
        "\n",
        "### Â¿QuÃ© son stopwords?\n",
        "Son palabras muy frecuentes que **aportan poco valor semÃ¡ntico**:\n",
        "- Preposiciones: \"de\", \"en\", \"con\", \"para\"\n",
        "- ArtÃ­culos: \"el\", \"la\", \"los\", \"las\"\n",
        "- Conjunciones: \"y\", \"o\", \"pero\"\n",
        "\n",
        "### ðŸŽ¯ Razones para eliminarlos:\n",
        "1. **Reducen \"ruido\"**: Aparecen en casi todo, no discriminan\n",
        "2. **Mejoran eficiencia**: Menos datos para procesar\n",
        "3. **Destacan tÃ©rminos clÃ­nicos**: Las palabras importantes resaltan mÃ¡s\n",
        "\n",
        "### ðŸ’¡ Ventaja en medicina:\n",
        "Mantiene tÃ©rminos clÃ­nicos relevantes: **paciente**, **fiebre**, **neumonÃ­a**, **tratamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ac2593",
      "metadata": {
        "id": "e0ac2593"
      },
      "outputs": [],
      "source": [
        "# PASO 5: Eliminar stopwords en espaÃ±ol\n",
        "stopwords_es = {\n",
        "    'de', 'y', 'con', 'el', 'la', 'los', 'las', 'en', 'un', 'una',\n",
        "    'unos', 'unas', 'es', 'son', 'estÃ¡', 'estÃ¡n', 'fue', 'fueron',\n",
        "    'se', 'por', 'para', 'que', 'a', 'o', 'e', 'al', 'del', 'sido',\n",
        "    'siendo', 'ha', 'han', 'haber', 'habÃ­a', 'habÃ­an', 'hay'\n",
        "}\n",
        "\n",
        "tokens_filtrados = [t for t in tokens if t not in stopwords_es and len(t) > 0]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DESPUÃ‰S DE ELIMINAR STOPWORDS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Tokens originales: {len(tokens)}\")\n",
        "print(f\"Tokens despuÃ©s del filtrado: {len(tokens_filtrados)}\")\n",
        "print(f\"Tokens eliminados: {len(tokens) - len(tokens_filtrados)}\\n\")\n",
        "\n",
        "print(\"Tokens relevantes (filtrados):\")\n",
        "print(tokens_filtrados)\n",
        "\n",
        "print(f\"\\nðŸ“Š COMPARACIÃ“N:\")\n",
        "print(f\"   â€¢ ReducciÃ³n: {((len(tokens) - len(tokens_filtrados)) / len(tokens) * 100):.1f}%\")\n",
        "print(f\"   â€¢ Vocabulario ahora: {len(set(tokens_filtrados))} palabras clÃ­nicas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dff69f1",
      "metadata": {
        "id": "3dff69f1"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª PASO 6 â€” REPRESENTACIÃ“N NUMÃ‰RICA: CONVERTIR PALABRAS A NÃšMEROS\n",
        "\n",
        "### Â¿Por quÃ© nÃºmeros?\n",
        "**Las mÃ¡quinas entienden nÃºmeros, no palabras.** Necesitamos convertir texto a vectores:\n",
        "\n",
        "| MÃ©todo | DescripciÃ³n | Caso de uso |\n",
        "|--------|-------------|-----------|\n",
        "| **One-Hot Encoding** | Vector binario para cada palabra | Vocabulario pequeÃ±o |\n",
        "| **Bag of Words (BoW)** | Cuenta de frecuencias | AnÃ¡lisis de tema |\n",
        "| **TF-IDF** | Importancia ponderada | BÃºsqueda relevancia |\n",
        "| **Word Embeddings** | Vectores densos (Word2Vec, GloVe) | SemÃ¡ntica profunda |\n",
        "\n",
        "Para esta clase, usaremos **Bag of Words** (mÃ©todo clÃ¡sico y didÃ¡ctico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05b348c",
      "metadata": {
        "id": "a05b348c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# PASO 6A: Crear vocabulario (mapeo palabra â†’ Ã­ndice)\n",
        "vocab = sorted(list(set(tokens_filtrados)))\n",
        "palabra_a_indice = {palabra: idx for idx, palabra in enumerate(vocab)}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VOCABULARIO GENERADO:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total de palabras Ãºnicas: {len(vocab)}\\n\")\n",
        "print(\"Primeras 15 palabras con Ã­ndices asignados:\")\n",
        "for i, palabra in enumerate(vocab[:15]):\n",
        "    print(f\"  {palabra_a_indice[palabra]:2d} â†’ {palabra}\")\n",
        "\n",
        "if len(vocab) > 15:\n",
        "    print(f\"  ... ({len(vocab) - 15} mÃ¡s)\\n\")\n",
        "else:\n",
        "    print()\n",
        "\n",
        "print(\"ðŸ”¢ RepresentaciÃ³n numÃ©rica:\")\n",
        "print(f\"   â€¢ Cada palabra es un Ã­ndice del 0 al {len(vocab)-1}\")\n",
        "print(f\"   â€¢ Estamos creando el 'diccionario' que la mÃ¡quina entiende\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a673f05",
      "metadata": {
        "id": "3a673f05"
      },
      "outputs": [],
      "source": [
        "# PASO 6B: Bag of Words (BoW)\n",
        "# Contar frecuencia de cada palabra\n",
        "bow_vector = np.zeros(len(vocab))\n",
        "for token in tokens_filtrados:\n",
        "    bow_vector[palabra_a_indice[token]] += 1\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BAG OF WORDS (BoW) - VECTOR DE FRECUENCIAS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"DimensiÃ³n del vector: {len(bow_vector)}\\n\")\n",
        "\n",
        "# Mostrar palabras con frecuencia > 0\n",
        "palabras_presentes = [(vocab[i], int(bow_vector[i]))\n",
        "                      for i in range(len(vocab))\n",
        "                      if bow_vector[i] > 0]\n",
        "\n",
        "palabras_presentes.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Palabras y sus frecuencias:\")\n",
        "for palabra, freq in palabras_presentes:\n",
        "    print(f\"  {palabra:20s} : {int(freq):2d} {'â–ˆ' * int(freq)}\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ InterpretaciÃ³n:\")\n",
        "print(f\"   â€¢ Vector de {len(bow_vector)} dimensiones\")\n",
        "print(f\"   â€¢ Cada posiciÃ³n = frecuencia de una palabra\")\n",
        "print(f\"   â€¢ Hemos convertido TEXTO â†’ NÃšMEROS que la mÃ¡quina procesa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a665400",
      "metadata": {
        "id": "3a665400"
      },
      "outputs": [],
      "source": [
        "# PASO 6C: Visualizar como matriz\n",
        "df_bow = pd.DataFrame(\n",
        "    bow_vector.reshape(1, -1),\n",
        "    columns=vocab\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"REPRESENTACIÃ“N EN DATAFRAME:\")\n",
        "print(\"=\"*60)\n",
        "print(df_bow)\n",
        "\n",
        "print(f\"\\nðŸ’¡ ExplicaciÃ³n:\")\n",
        "print(f\"   â€¢ Fila = 1 documento (nuestra nota clÃ­nica)\")\n",
        "print(f\"   â€¢ Columnas = {len(vocab)} palabras del vocabulario\")\n",
        "print(f\"   â€¢ Valores = frecuencia de cada palabra\")\n",
        "print(f\"\\n   Si tuviÃ©ramos mÃºltiples documentos, tendrÃ­amos mÃºltiples filas.\")\n",
        "print(f\"   Esto es la BASE de cÃ³mo la IA procesa textos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f693f60",
      "metadata": {
        "id": "2f693f60"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”„ FLUJO COMPLETO: DEL TEXTO CRUDO AL VECTOR NUMÃ‰RICO\n",
        "\n",
        "### Visualizar la transformaciÃ³n paso a paso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7064ca",
      "metadata": {
        "id": "4b7064ca"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TRANSFORMACIÃ“N COMPLETA: TEXTO CRUDO â†’ VECTOR NUMÃ‰RICO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nðŸ“ ENTRADA (Texto Crudo):\")\n",
        "print(\"-\" * 70)\n",
        "print(texto_crudo[:150] + \"...\\n\")\n",
        "\n",
        "print(\"1ï¸âƒ£ NORMALIZACIÃ“N (minÃºsculas):\")\n",
        "print(\"-\" * 70)\n",
        "print(texto_normalizado[:150] + \"...\\n\")\n",
        "\n",
        "print(\"2ï¸âƒ£ LIMPIEZA (eliminar sÃ­mbolos):\")\n",
        "print(\"-\" * 70)\n",
        "print(texto_limpio[:150] + \"...\\n\")\n",
        "\n",
        "print(\"3ï¸âƒ£ TOKENIZACIÃ“N (dividir en palabras):\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{tokens[:10]}...\\n\")\n",
        "print(f\"Total tokens: {len(tokens)}\\n\")\n",
        "\n",
        "print(\"4ï¸âƒ£ FILTRADO (eliminar stopwords):\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{tokens_filtrados}...\\n\")\n",
        "print(f\"Total tokens relevantes: {len(tokens_filtrados)}\\n\")\n",
        "\n",
        "print(\"5ï¸âƒ£ VECTORIZACIÃ“N (Bag of Words):\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Vector numÃ©rico de dimensiÃ³n {len(vocab)}:\")\n",
        "print(f\"{bow_vector}\\n\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… LISTO: El texto clÃ­nico es ahora un vector que la IA puede procesar\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170968b2",
      "metadata": {
        "id": "170968b2"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª CASO PRÃCTICO ADICIONAL: PROCESAR MÃšLTIPLES DOCUMENTOS\n",
        "\n",
        "Simulemos procesamiento de varias notas clÃ­nicas simultÃ¡neamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb58d27",
      "metadata": {
        "id": "ffb58d27"
      },
      "outputs": [],
      "source": [
        "# MÃºltiples documentos clÃ­nicos\n",
        "documentos = [\n",
        "    \"\"\"Paciente femenino 45 aÃ±os diabetes mellitus hipertensiÃ³n.\n",
        "    Presenta glucosa 285 mg/dl, presiÃ³n arterial 160/95.\"\"\",\n",
        "\n",
        "    \"\"\"VarÃ³n 72 aÃ±os EPOC bronquitis crÃ³nica.\n",
        "    SaturaciÃ³n oxÃ­geno 88%, requiere oxÃ­geno suplementario.\"\"\",\n",
        "\n",
        "    \"\"\"Mujer 55 aÃ±os cÃ¡ncer mama metastÃ¡sico.\n",
        "    Hemoglobina 9.2 g/dl, tratamiento quimioterapia iniciado.\"\"\"\n",
        "]\n",
        "\n",
        "# Procesar todos los documentos\n",
        "matriz_documentos = []\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PROCESAMIENTO DE MÃšLTIPLES DOCUMENTOS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for idx, doc in enumerate(documentos, 1):\n",
        "    # Aplicar el mismo pipeline\n",
        "    doc_norm = doc.lower()\n",
        "    doc_limpio = re.sub(r\"[^a-zÃ¡Ã©Ã­Ã³ÃºÃ±0-9\\s]\", \"\", doc_norm)\n",
        "    doc_limpio = re.sub(r\"\\s+\", \" \", doc_limpio).strip()\n",
        "    doc_tokens = doc_limpio.split()\n",
        "    doc_tokens_filt = [t for t in doc_tokens if t not in stopwords_es and len(t) > 0]\n",
        "\n",
        "    # Crear vector BoW para este documento\n",
        "    vec = np.zeros(len(vocab))\n",
        "    for token in doc_tokens_filt:\n",
        "        if token in palabra_a_indice:\n",
        "            vec[palabra_a_indice[token]] += 1\n",
        "\n",
        "    matriz_documentos.append(vec)\n",
        "    print(f\"\\nðŸ“„ Documento {idx}:\")\n",
        "    print(f\"   Texto: {doc[:60]}...\")\n",
        "    print(f\"   Tokens clÃ­nicos: {doc_tokens_filt}\")\n",
        "\n",
        "# Crear matriz final\n",
        "matriz_final = np.array(matriz_documentos)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MATRIZ FINAL: Documentos Ã— Vocabulario\")\n",
        "print(\"=\"*70)\n",
        "df_final = pd.DataFrame(matriz_final, columns=vocab)\n",
        "print(df_final)\n",
        "\n",
        "print(f\"\\nðŸ’¡ InterpretaciÃ³n:\")\n",
        "print(f\"   â€¢ DimensiÃ³n: {matriz_final.shape[0]} documentos Ã— {matriz_final.shape[1]} palabras\")\n",
        "print(f\"   â€¢ Esta es la entrada tÃ­pica para modelos de IA (clasificaciÃ³n, clustering, etc.)\")\n",
        "print(f\"   â€¢ Cada fila = documento procesado y vectorizado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c8a340",
      "metadata": {
        "id": "19c8a340"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š ANÃLISIS Y CONCLUSIONES\n",
        "\n",
        "### Â¿QuÃ© hemos aprendido?\n",
        "\n",
        "1. **Texto Crudo** â†’ Realidad de los datos mÃ©dicos (desordenado, con sÃ­mbolos)\n",
        "2. **NormalizaciÃ³n** â†’ EstandarizaciÃ³n para reducir variabilidad\n",
        "3. **Limpieza** â†’ EliminaciÃ³n de ruido sin perder informaciÃ³n clÃ­nica\n",
        "4. **TokenizaciÃ³n** â†’ ConversiÃ³n a unidades analizables\n",
        "5. **Filtrado** â†’ Enfoque en palabras relevantes (stopword removal)\n",
        "6. **VectorizaciÃ³n** â†’ TransformaciÃ³n a nÃºmeros para mÃ¡quinas\n",
        "\n",
        "### Flujo resumido:\n",
        "```\n",
        "TEXTO CRUDO\n",
        "    â†“\n",
        "NORMALIZAR (minÃºsculas)\n",
        "    â†“\n",
        "LIMPIAR (sÃ­mbolos)\n",
        "    â†“\n",
        "TOKENIZAR (palabras)\n",
        "    â†“\n",
        "FILTRAR (stopwords)\n",
        "    â†“\n",
        "VECTORIZAR (nÃºmeros)\n",
        "    â†“\n",
        "VECTOR NUMÃ‰RICO â† Listo para IA\n",
        "```\n",
        "\n",
        "### ðŸŽ¯ AplicaciÃ³n en Salud:\n",
        "- **ClasificaciÃ³n**: Â¿QuÃ© diagnÃ³stico sugiere este texto?\n",
        "- **ExtracciÃ³n de entidades**: Â¿DÃ³nde estÃ¡n los sÃ­ntomas, medicamentos, etc.?\n",
        "- **Agrupamiento**: Â¿QuÃ© pacientes tienen perfiles clÃ­nicos similares?\n",
        "- **PredicciÃ³n**: Â¿CuÃ¡l es el riesgo o pronÃ³stico?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b132bf86",
      "metadata": {
        "id": "b132bf86"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸš€ SIGUIENTE PASO\n",
        "\n",
        "En la prÃ³xima clase exploraremos:\n",
        "- **TF-IDF**: PonderaciÃ³n inteligente (palabras mÃ¡s discriminativas pesan mÃ¡s)\n",
        "- **Word Embeddings**: Capturar semÃ¡ntica y relaciones entre palabras\n",
        "- **Redes neuronales**: Procesamiento automÃ¡tico de texto (NLP avanzado)\n",
        "- **Transformers**: BERT, GPT y modelos modernos\n",
        "\n",
        "### ðŸ’¡ ReflexiÃ³n final:\n",
        "\n",
        "> **\"Hemos convertido palabras mÃ©dicas en nÃºmeros.\"**\n",
        "> **\"Ahora, cualquier algoritmo de aprendizaje automÃ¡tico puede entenderlas.\"**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“š Referencias y recursos\n",
        "\n",
        "- **NLTK**: Natural Language Toolkit para Python\n",
        "- **Scikit-learn**: Machine Learning con TfidfVectorizer, CountVectorizer\n",
        "- **SpaCy**: NLP industrial-grade\n",
        "- **Transformers (Hugging Face)**: Modelos pre-entrenados para medicina\n",
        "\n",
        "---\n",
        "\n",
        "**Fin de la Clase PrÃ¡ctica** âœ…"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}